{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Atlas dataset into Oceantide format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import itertools\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "from oceantide import read_otis_atlas_netcdf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chunked_iterable(iterable, size):\n",
    "    \"\"\"Iterate through array over chunks with specific size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert by iterating through longitudes over chunks of size 200 to handle memory\n",
    "The Atlas dataset is very large. It is composed by one file per constituent for elevation and transports at 1/30 degree resolution. Reading in the full dataset into oceantide format cannot be handled unless available RAM is very large. Here we overcome this limitation by converting and concatenating longitude chunks at a time. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "atlas_path = Path(\"/data/tide/tpxo9v4_atlas/TPXO9_atlas_nc\")\n",
    "\n",
    "lons = xr.open_dataset(atlas_path / \"grid_tpxo9_atlas_v4.nc\").lon_z.values\n",
    "\n",
    "chunksizes = 100\n",
    "itersize = 200\n",
    "\n",
    "for ind in chunked_iterable(range(lons.size), size=itersize):\n",
    "    x0 = lons[ind[0]]\n",
    "    x1 = lons[ind[-1]]\n",
    "    print(f\"Writing lon chunk {ind[0]}\")\n",
    "    dset = read_otis_atlas_netcdf(atlas_path, x0=x0, x1=x1, nxchunk=itersize, nychunk=None)\n",
    "    dset = dset.chunk({\"con\": None, \"lon\": chunksizes, \"lat\": chunksizes})\n",
    "    if ind[0] == 0:\n",
    "        mode = \"w\"\n",
    "        append_dim = None\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "        append_dim = \"lon\"\n",
    "    dset.to_zarr(\"./atlas.zarr\", mode=mode, consolidated=True, append_dim=append_dim)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('oceantide': virtualenv)"
  },
  "interpreter": {
   "hash": "6956ea67edaf5112b6260d91a9ed58293cc492e70f651f1c61c27d0c2fe2e7c0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}